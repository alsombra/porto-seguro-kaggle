{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# This prints out (rows, columns) in each dataframe\n",
    "print('Train shape:', df_train.shape)\n",
    "print('Test shape:', df_test.shape)\n",
    "\n",
    "print('Columns:', df_train.columns)\n",
    "\n",
    "y_train = df_train['target'].values\n",
    "id_train = df_train['id'].values\n",
    "id_test = df_test['id'].values\n",
    "\n",
    "# People are saying these colunms are useless, but not sure if we should really drop them\n",
    "col_to_drop = df_train.columns[df_train.columns.str.startswith('ps_calc_')]\n",
    "df_train = df_train.drop(col_to_drop, axis=1)  \n",
    "df_test = df_test.drop(col_to_drop, axis=1)  \n",
    "\n",
    "# We drop these variables as we don't want to train on them\n",
    "# The other 57 columns are all numerical and can be trained on without preprocessing\n",
    "x_train = df_train.drop(['target', 'id'], axis=1)\n",
    "x_test = df_test.drop(['id'], axis=1)\n",
    "\n",
    "len_train = len(x_train)\n",
    "len_test = len(x_test)\n",
    "\n",
    "x_all = pd.concat([x_train,x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/tilii7/dimensionality-reduction-pca-tsne\n",
    "\n",
    "#I was trying adding PCA and ICA columns. Although they have a lot of variance explaining \n",
    "#power, it doesn't seem to help the model. However, it doesn't make it worse too.\n",
    "\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "\n",
    "n_comp = 5\n",
    "# PCA\n",
    "print('\\nRunning PCA ...')\n",
    "pca = PCA(n_components=n_comp, svd_solver='full', random_state=1001)\n",
    "x_pca = pca.fit_transform(x_all)\n",
    "print('Explained variance: %.4f' % pca.explained_variance_ratio_.sum())\n",
    "\n",
    "print('Individual variance contributions:')\n",
    "for j in range(n_comp):\n",
    "    print(pca.explained_variance_ratio_[j])\n",
    "    \n",
    "\n",
    "#ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=1001)\n",
    "ica2_results = ica.fit_transform(x_all)\n",
    "    \n",
    "for i in range(1, n_comp+1):\n",
    "    x_all['pca_' + str(i)] = x_pca[:,i-1]\n",
    "    \n",
    "    x_all['ica_' + str(i)] = ica2_results[:,i-1]\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns:', x_all.columns)\n",
    "\n",
    "x_train = x_all.head(len_train)\n",
    "x_test = x_all.tail(len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Puro\n",
    "\n",
    "# Take a random 20% of the dataset as validation data\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n",
    "print('Train samples: {} Validation samples: {}'.format(len(x_train), len(x_valid)))\n",
    "\n",
    "# Convert our data into XGBoost format\n",
    "d_train = xgb.DMatrix(x_train, y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, y_valid)\n",
    "d_test = xgb.DMatrix(x_test)\n",
    "\n",
    "# Set xgboost parameters\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eta'] = 0.02\n",
    "params['silent'] = True\n",
    "params['max_depth'] = 6\n",
    "params['subsample'] = 0.9\n",
    "params['colsample_bytree'] = 0.9\n",
    "\n",
    "# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "# Create an XGBoost-compatible metric from Gini\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return [('gini', gini_score)]\n",
    "\n",
    "# This is the data xgboost will test on after eachboosting round\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "# Train the model! We pass in a max of 10,000 rounds (with early stopping after 100)\n",
    "# and the custom metric (maximize=True tells xgb that higher metric is better)\n",
    "mdl = xgb.train(params, d_train, 10000, watchlist, early_stopping_rounds=100, feval=gini_xgb, maximize=True, verbose_eval=10)\n",
    "\n",
    "# Predict on our test data\n",
    "p_test = mdl.predict(d_test)\n",
    "\n",
    "# Create a submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = p_test\n",
    "sub.to_csv('xgb1.csv', index=False)\n",
    "\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case you ran the pure XGBoost above\n",
    "x_train = x_all.head(len_train)\n",
    "x_test = x_all.tail(len_test)\n",
    "\n",
    "\n",
    "# XGBoost emsemble w/ LightLGB using K-fold for validation\n",
    "def gini(y, pred):\n",
    "    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
    "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(y) + 1) / 2.\n",
    "    return gs / len(y)\n",
    "\n",
    "def gini_xgb(pred, y):\n",
    "    y = y.get_label()\n",
    "    return 'gini', gini(y, pred) / gini(y, y)\n",
    "\n",
    "def gini_lgb(preds, dtrain):\n",
    "    y = list(dtrain.get_label())\n",
    "    score = gini(y, preds) / gini(y, y)\n",
    "    return 'gini', score, True\n",
    "\n",
    "\n",
    "\n",
    "# xgb\n",
    "params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n",
    "          'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n",
    "\n",
    "X = x_train\n",
    "features = X.columns\n",
    "X = X.values\n",
    "y = df_train['target'].values\n",
    "sub=df_test['id'].to_frame()\n",
    "sub['target']=0\n",
    "\n",
    "nrounds=2000  # need to change to 2000\n",
    "kfold = 5  # need to change to 5\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=0)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    d_train = xgb.DMatrix(X_train, y_train) \n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid) \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    xgb_model = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n",
    "                          feval=gini_xgb, maximize=True, verbose_eval=10)\n",
    "    sub['target'] += xgb_model.predict(xgb.DMatrix(x_test[features].values), \n",
    "                        ntree_limit=xgb_model.best_ntree_limit+50) / (2*kfold)\n",
    "gc.collect()\n",
    "sub.head(2)\n",
    "\n",
    "# lgb\n",
    "params = {'metric': 'auc', 'learning_rate' : 0.01, 'max_depth':5, 'num_leaves': 10, 'max_bin':10,  'objective': 'binary', \n",
    "          'feature_fraction': 0.8,'bagging_fraction':0.9,'bagging_freq':10,  'min_data': 500}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=1)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\n",
    "    X_train, X_eval = X[train_index], X[test_index]\n",
    "    y_train, y_eval = y[train_index], y[test_index]\n",
    "    lgb_model = lgb.train(params, lgb.Dataset(X_train, label=y_train), nrounds, \n",
    "                  lgb.Dataset(X_eval, label=y_eval), verbose_eval=10, \n",
    "                  feval=gini_lgb, early_stopping_rounds=100)\n",
    "    sub['target'] += lgb_model.predict(x_test[features].values, \n",
    "                        num_iteration=lgb_model.best_iteration) / (2*kfold)\n",
    "    \n",
    "sub.to_csv('sub10.csv', index=False, float_format='%.5f') \n",
    "gc.collect()\n",
    "sub.head(2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
